"""
Cognitive Benchmark Service - ì´ˆê¸° ì¸ì§€ ì‹¤í—˜ ì„¤ê³„ ë° ì‹¤í–‰

ë§ê° ê³¡ì„  ì¶”ì •ì„ ìœ„í•œ ì´ˆê¸° ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸:
- ìµìˆ™í•œ ë„ë©”ì¸ (CS) vs ìµìˆ™í•˜ì§€ ì•Šì€ ë„ë©”ì¸ (ê²½ìƒë„ ì‚¬íˆ¬ë¦¬)
- T0~T3 ì‹œê°„ì¶• ê¸°ë°˜ ê¸°ì–µ í˜•ì„±/ë¶•ê´´ ì¸¡ì •
- illusion_score, RT ê¸°ë°˜ ì¸ì§€ íŠ¹ì„± ë¶„ì„
"""

import math
import logging
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum

from django.db import models
from django.db.models import Avg, Count, Q
from django.utils import timezone

logger = logging.getLogger(__name__)


# =============================================================================
# ë„ë©”ì¸ ì •ì˜
# =============================================================================

class Domain(Enum):
    """í…ŒìŠ¤íŠ¸ ë„ë©”ì¸"""
    CS = "cs"                    # ìµìˆ™í•œ ë„ë©”ì¸ (Computer Science)
    DIALECT = "dialect"          # ìµìˆ™í•˜ì§€ ì•Šì€ ë„ë©”ì¸ (ê²½ìƒë„ ì‚¬íˆ¬ë¦¬)


# CS ë„ë©”ì¸ íƒœê·¸
CS_TAGS = [
    "ë°ì´í„°ë² ì´ìŠ¤", "ìš´ì˜ì²´ì œ", "ìë£Œêµ¬ì¡°", "ë„¤íŠ¸ì›Œí¬", "ì•Œê³ ë¦¬ì¦˜",
    "ì»´í“¨í„°ê³¼í•™", "í”„ë¡œê·¸ë˜ë°", "ì†Œí”„íŠ¸ì›¨ì–´", "ì‹œìŠ¤í…œ",
    "database", "os", "data_structure", "network", "algorithm"
]

# ê²½ìƒë„ ì‚¬íˆ¬ë¦¬ ë„ë©”ì¸ íƒœê·¸
DIALECT_TAGS = [
    "ê²½ìƒë„", "ì‚¬íˆ¬ë¦¬", "ë°©ì–¸", "ì§€ì—­í‘œí˜„", "êµ¬ì–´ì²´",
    "ê²½ìƒë„_ì‚¬íˆ¬ë¦¬", "ê²½ìƒë„_ë°©ì–¸", "dialect"
]


# =============================================================================
# ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤
# =============================================================================

@dataclass
class TimePointStats:
    """ì‹œì ë³„ í†µê³„"""
    time_point: str
    accuracy: float          # ì •ë‹µë¥  (0.0 ~ 1.0)
    avg_rt_ms: float         # í‰ê·  ë°˜ì‘ ì‹œê°„ (ms)
    avg_confidence: float    # í‰ê·  í™•ì‹ ë„
    avg_illusion: float      # í‰ê·  ì°©ê° ì§€ìˆ˜
    sample_count: int        # ìƒ˜í”Œ ìˆ˜


@dataclass
class DomainAnalysis:
    """ë„ë©”ì¸ë³„ ë¶„ì„ ê²°ê³¼"""
    domain: str
    time_point_stats: List[TimePointStats]
    forgetting_k: float               # ë§ê° ê¸°ìš¸ê¸°
    encoding_strength: float          # ì´ˆê¸° ê¸°ì–µ í˜•ì„± ê°•ë„
    retention_rate_t3: float          # T3 ì‹œì  ìœ ì§€ìœ¨
    avg_illusion_score: float         # í‰ê·  ì°©ê° ì§€ìˆ˜
    illusion_tendency: str            # ê³¼ì‹ /ì‹ ì¤‘/ê· í˜•


@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ì „ì²´ ê²°ê³¼"""
    user_id: str
    cs_analysis: DomainAnalysis
    dialect_analysis: DomainAnalysis
    base_forgetting_k: float          # ì¶”ì •ëœ ê¸°ì´ˆ ë§ê° ìƒìˆ˜
    domain_forgetting_ratio: float    # k_dialect / k_cs (ë„ë©”ì¸ ì°¨ì´)
    overall_illusion_avg: float       # ì „ì²´ í‰ê·  ì°©ê° ì§€ìˆ˜
    recommendation: str               # ë³µìŠµ ìŠ¤ì¼€ì¤„ ê¶Œì¥ì‚¬í•­


# =============================================================================
# ë³µìŠµ ìŠ¤ì¼€ì¤„ ê³„ì‚°
# =============================================================================

def calculate_next_review_hours(k: float, target_retention: float) -> float:
    """
    ëª©í‘œ ì•”ê¸°ìœ¨ì„ ë§Œì¡±í•˜ëŠ” ë‹¤ìŒ ë³µìŠµ ì‹œì  ê³„ì‚°
    
    ì—ë¹™í•˜ìš°ìŠ¤ ë§ê° ê³¡ì„  ê³µì‹:
        R(t) = exp(-k * t)
    
    ì—­ì‚°í•˜ì—¬ të¥¼ êµ¬í•¨:
        t = -ln(R_target) / k
    
    Args:
        k: ë§ê° ê³„ìˆ˜ (ë‹¨ìœ„: 1/hour, k > 0)
        target_retention: ëª©í‘œ ì•”ê¸°ìœ¨ (0 < R < 1)
        
    Returns:
        ë‹¤ìŒ ë³µìŠµê¹Œì§€ì˜ ì‹œê°„ (hours)
        
    Raises:
        ValueError: k <= 0 ë˜ëŠ” target_retentionì´ (0, 1) ë²”ìœ„ ë°–ì¼ ê²½ìš°
        
    Examples:
        >>> calculate_next_review_hours(k=0.0213, target_retention=0.8)
        10.48  # ì•½ 10.5ì‹œê°„ í›„ ë³µìŠµ
        
        >>> calculate_next_review_hours(k=0.132, target_retention=0.8)
        1.69   # ì•½ 1.7ì‹œê°„ í›„ ë³µìŠµ
    """
    # ì…ë ¥ ê²€ì¦
    if k <= 0:
        raise ValueError(f"ë§ê° ê³„ìˆ˜ këŠ” ì–‘ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤: k={k}")
    
    if not (0 < target_retention < 1):
        raise ValueError(
            f"ëª©í‘œ ì•”ê¸°ìœ¨ì€ 0ê³¼ 1 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤: target_retention={target_retention}"
        )
    
    # t = -ln(R_target) / k
    # ln(0.8) = -0.223, so t = 0.223 / k
    t_next = -math.log(target_retention) / k
    
    return t_next


@dataclass
class ReviewSchedule:
    """ë³µìŠµ ìŠ¤ì¼€ì¤„ ì •ë³´"""
    target_retention: float           # ëª©í‘œ ì•”ê¸°ìœ¨
    cs_review_hours: float            # CS ë„ë©”ì¸ ë³µìŠµ ì‹œì  (ì‹œê°„)
    dialect_review_hours: float       # ì‚¬íˆ¬ë¦¬ ë„ë©”ì¸ ë³µìŠµ ì‹œì  (ì‹œê°„)
    cs_review_datetime: Optional[datetime] = None    # CS ë³µìŠµ ì‹œê°
    dialect_review_datetime: Optional[datetime] = None  # ì‚¬íˆ¬ë¦¬ ë³µìŠµ ì‹œê°


class ReviewScheduleCalculator:
    """
    ë³µìŠµ ìŠ¤ì¼€ì¤„ ê³„ì‚°ê¸°
    
    ë§ê° ê³¡ì„  ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ ë³µìŠµ ì‹œì ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    
    # ê¸°ë³¸ ëª©í‘œ ì•”ê¸°ìœ¨
    DEFAULT_TARGET_RETENTION = 0.8
    
    # ì•”ê¸°ìœ¨ë³„ ê¶Œì¥ ë³µìŠµ ê°„ê²© ì„ê³„ê°’
    RETENTION_THRESHOLDS = {
        'high': 0.9,      # ë†’ì€ ìœ ì§€ìœ¨ ëª©í‘œ
        'medium': 0.8,    # ì¤‘ê°„ ìœ ì§€ìœ¨ ëª©í‘œ (ê¸°ë³¸)
        'low': 0.7,       # ë‚®ì€ ìœ ì§€ìœ¨ í—ˆìš©
    }
    
    def calculate_review_schedule(
        self,
        k_cs: float,
        k_dialect: float,
        target_retention: float = None,
        from_time: datetime = None
    ) -> ReviewSchedule:
        """
        ë„ë©”ì¸ë³„ ë³µìŠµ ìŠ¤ì¼€ì¤„ ê³„ì‚°
        
        Args:
            k_cs: CS ë„ë©”ì¸ ë§ê° ê³„ìˆ˜
            k_dialect: ì‚¬íˆ¬ë¦¬ ë„ë©”ì¸ ë§ê° ê³„ìˆ˜
            target_retention: ëª©í‘œ ì•”ê¸°ìœ¨ (ê¸°ë³¸: 0.8)
            from_time: ê¸°ì¤€ ì‹œê°„ (ê¸°ë³¸: í˜„ì¬)
            
        Returns:
            ReviewSchedule ê°ì²´
        """
        if target_retention is None:
            target_retention = self.DEFAULT_TARGET_RETENTION
        
        if from_time is None:
            from_time = timezone.now()
        
        # ê° ë„ë©”ì¸ë³„ ë³µìŠµ ì‹œê°„ ê³„ì‚°
        cs_hours = calculate_next_review_hours(k_cs, target_retention)
        dialect_hours = calculate_next_review_hours(k_dialect, target_retention)
        
        # ë³µìŠµ ì‹œê° ê³„ì‚°
        cs_datetime = from_time + timedelta(hours=cs_hours)
        dialect_datetime = from_time + timedelta(hours=dialect_hours)
        
        return ReviewSchedule(
            target_retention=target_retention,
            cs_review_hours=round(cs_hours, 2),
            dialect_review_hours=round(dialect_hours, 2),
            cs_review_datetime=cs_datetime,
            dialect_review_datetime=dialect_datetime
        )
    
    def calculate_multi_retention_schedules(
        self,
        k_cs: float,
        k_dialect: float
    ) -> Dict[str, ReviewSchedule]:
        """
        ì—¬ëŸ¬ ëª©í‘œ ì•”ê¸°ìœ¨ì— ëŒ€í•œ ë³µìŠµ ìŠ¤ì¼€ì¤„ ê³„ì‚°
        
        Returns:
            {'high': ReviewSchedule, 'medium': ReviewSchedule, 'low': ReviewSchedule}
        """
        return {
            level: self.calculate_review_schedule(k_cs, k_dialect, retention)
            for level, retention in self.RETENTION_THRESHOLDS.items()
        }
    
    def format_hours_to_human_readable(self, hours: float) -> str:
        """
        ì‹œê°„ì„ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        
        Examples:
            0.5 -> "30ë¶„"
            1.5 -> "1ì‹œê°„ 30ë¶„"
            25.0 -> "1ì¼ 1ì‹œê°„"
        """
        if hours < 1:
            minutes = int(hours * 60)
            return f"{minutes}ë¶„"
        elif hours < 24:
            h = int(hours)
            m = int((hours - h) * 60)
            if m > 0:
                return f"{h}ì‹œê°„ {m}ë¶„"
            return f"{h}ì‹œê°„"
        else:
            days = int(hours / 24)
            remaining_hours = int(hours % 24)
            if remaining_hours > 0:
                return f"{days}ì¼ {remaining_hours}ì‹œê°„"
            return f"{days}ì¼"


# =============================================================================
# ë§ê°ê³¡ì„  ì‹œê°í™” ë°ì´í„° ìƒì„±
# =============================================================================

# ê¸°ë³¸ ì‹œê°„ ìƒ˜í”Œë§ í¬ì¸íŠ¸ (ì‹œê°„ ë‹¨ìœ„)
DEFAULT_CURVE_TIME_POINTS = [0, 1, 3, 6, 12, 24, 48]


def calculate_retention(k: float, t: float) -> float:
    """
    íŠ¹ì • ì‹œì ì˜ ê¸°ì–µ ìœ ì§€ìœ¨ ê³„ì‚°
    
    Ebbinghaus ë§ê° ê³¡ì„ : R(t) = exp(-k * t)
    
    Args:
        k: ë§ê° ê³„ìˆ˜ (1/hour)
        t: ê²½ê³¼ ì‹œê°„ (hour)
        
    Returns:
        ê¸°ì–µ ìœ ì§€ìœ¨ (0.0 ~ 1.0)
    """
    return math.exp(-k * t)


def generate_forgetting_curve(
    k: float, 
    time_points: List[float] = None
) -> List[Dict[str, float]]:
    """
    ë§ê°ê³¡ì„  ì‹œê°í™”ìš© ì¢Œí‘œ ë°ì´í„° ìƒì„±
    
    í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì§ì ‘ ê·¸ë˜í”„ë¥¼ ê·¸ë¦´ ìˆ˜ ìˆë„ë¡
    (t, retention) ì¢Œí‘œ ë°°ì—´ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        k: ë§ê° ê³„ìˆ˜ (1/hour)
        time_points: ìƒ˜í”Œë§í•  ì‹œê°„ í¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸: [0,1,3,6,12,24,48])
        
    Returns:
        [{"t": 0, "retention": 1.0}, {"t": 6, "retention": 0.89}, ...]
        
    Example:
        >>> generate_forgetting_curve(k=0.01, time_points=[0, 6, 12, 24])
        [
            {"t": 0, "retention": 1.0},
            {"t": 6, "retention": 0.942},
            {"t": 12, "retention": 0.887},
            {"t": 24, "retention": 0.787}
        ]
    """
    if time_points is None:
        time_points = DEFAULT_CURVE_TIME_POINTS
    
    curve_data = []
    for t in time_points:
        retention = calculate_retention(k, t)
        curve_data.append({
            "t": t,
            "retention": round(retention, 3)
        })
    
    return curve_data


@dataclass
class ForgettingCurveData:
    """ë§ê°ê³¡ì„  ì‹œê°í™” ë°ì´í„°"""
    cs: List[Dict[str, float]]
    dialect: List[Dict[str, float]]


class ForgettingCurveGenerator:
    """
    ë§ê°ê³¡ì„  ì‹œê°í™” ë°ì´í„° ìƒì„±ê¸°
    
    í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ìˆ˜ì‹ì„ ì•Œ í•„ìš” ì—†ì´
    ë°”ë¡œ ê·¸ë˜í”„ë¥¼ ë Œë”ë§í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    def __init__(self, time_points: List[float] = None):
        """
        Args:
            time_points: ì»¤ìŠ¤í…€ ìƒ˜í”Œë§ ì‹œê°„ í¬ì¸íŠ¸
        """
        self.time_points = time_points or DEFAULT_CURVE_TIME_POINTS
    
    def generate(
        self, 
        k_cs: float, 
        k_dialect: float
    ) -> ForgettingCurveData:
        """
        ë„ë©”ì¸ë³„ ë§ê°ê³¡ì„  ë°ì´í„° ìƒì„±
        
        Args:
            k_cs: CS ë„ë©”ì¸ ë§ê° ê³„ìˆ˜
            k_dialect: ì‚¬íˆ¬ë¦¬ ë„ë©”ì¸ ë§ê° ê³„ìˆ˜
            
        Returns:
            ForgettingCurveData ê°ì²´
        """
        return ForgettingCurveData(
            cs=generate_forgetting_curve(k_cs, self.time_points),
            dialect=generate_forgetting_curve(k_dialect, self.time_points)
        )
    
    def to_dict(
        self, 
        k_cs: float, 
        k_dialect: float
    ) -> Dict[str, List[Dict[str, float]]]:
        """
        JSON ì§ë ¬í™” ê°€ëŠ¥í•œ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
        
        Returns:
            {
                "cs": [{"t": 0, "retention": 1.0}, ...],
                "dialect": [{"t": 0, "retention": 1.0}, ...]
            }
        """
        data = self.generate(k_cs, k_dialect)
        return {
            "cs": data.cs,
            "dialect": data.dialect
        }


# =============================================================================
# ë…¸ë“œ ì„ ì • ì„œë¹„ìŠ¤
# =============================================================================

class NodeSelector:
    """
    í…ŒìŠ¤íŠ¸ ëŒ€ìƒ ë…¸ë“œ ì„ ì •
    
    ê° ë„ë©”ì¸ì—ì„œ ì ì ˆí•œ ë…¸ë“œë¥¼ ì„ íƒí•˜ì—¬ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ êµ¬ì„±
    
    ë¶„ë¥˜ ê¸°ì¤€:
    - CS (TRACK_A): Computer Science ê´€ë ¨ ë…¸ë“œ, ë°©ì–¸/ì‚¬íˆ¬ë¦¬ íƒœê·¸ ì œì™¸
    - Dialect (TRACK_B): ê²½ìƒë„/ì „ë¼ë„ ì‚¬íˆ¬ë¦¬, ë°©ì–¸ ê´€ë ¨ ë…¸ë“œ
    """
    
    # ë°©ì–¸/ì‚¬íˆ¬ë¦¬ ê´€ë ¨ íƒœê·¸ (CSì—ì„œ ì œì™¸í•´ì•¼ í•  íƒœê·¸)
    DIALECT_EXCLUDE_TAGS = [
        'ê²½ìƒë„', 'ì „ë¼ë„', 'ì‚¬íˆ¬ë¦¬', 'ë°©ì–¸', 'ì§€ì—­í‘œí˜„', 'êµ¬ì–´ì²´',
        'ê²½ìƒë„_ì‚¬íˆ¬ë¦¬', 'ê²½ìƒë„_ë°©ì–¸', 'ì „ë¼ë„_ì‚¬íˆ¬ë¦¬', 'ì „ë¼ë„_ë°©ì–¸',
        'dialect', 'í–¥í† ë¬¸í™”', 'ì§€ì—­ë¬¸í™”', 'ë¯¼ì†',
    ]
    
    # CS ê´€ë ¨ íƒœê·¸ (ìˆœìˆ˜ CS ë…¸ë“œ ì‹ë³„ìš©)
    CS_INCLUDE_TAGS = [
        'ë°ì´í„°ë² ì´ìŠ¤', 'ìš´ì˜ì²´ì œ', 'ìë£Œêµ¬ì¡°', 'ë„¤íŠ¸ì›Œí¬', 'ì•Œê³ ë¦¬ì¦˜',
        'ì»´í“¨í„°ê³¼í•™', 'í”„ë¡œê·¸ë˜ë°', 'ì†Œí”„íŠ¸ì›¨ì–´', 'ì‹œìŠ¤í…œ',
        'database', 'os', 'data_structure', 'network', 'algorithm',
        'computer_science', 'programming', 'software', 'machine_learning',
        'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹', 'deep_learning', 'api', 'backend', 'frontend',
    ]
    
    def __init__(self, nodes_per_domain: int = 10):
        """
        Args:
            nodes_per_domain: ë„ë©”ì¸ë‹¹ ì„ íƒí•  ë…¸ë“œ ìˆ˜ (ê¶Œì¥: 8~12)
        """
        self.nodes_per_domain = nodes_per_domain
    
    def _has_dialect_tags(self, node) -> bool:
        """ë…¸ë“œê°€ ë°©ì–¸/ì‚¬íˆ¬ë¦¬ ê´€ë ¨ íƒœê·¸ë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ í™•ì¸"""
        if not node.tags:
            return False
        
        node_tags_lower = [str(tag).lower() for tag in node.tags]
        for exclude_tag in self.DIALECT_EXCLUDE_TAGS:
            for node_tag in node_tags_lower:
                if exclude_tag.lower() in node_tag:
                    return True
        return False
    
    def _has_cs_tags(self, node) -> bool:
        """ë…¸ë“œê°€ CS ê´€ë ¨ íƒœê·¸ë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ í™•ì¸"""
        if not node.tags:
            return False
        
        node_tags_lower = [str(tag).lower() for tag in node.tags]
        for cs_tag in self.CS_INCLUDE_TAGS:
            for node_tag in node_tags_lower:
                if cs_tag.lower() in node_tag:
                    return True
        return False
    
    def select_cs_nodes(self) -> List:
        """
        CS ë„ë©”ì¸ ë…¸ë“œ ì„ ì •
        
        ì„ ì • ê¸°ì¤€ (ìš°ì„ ìˆœìœ„):
        1. TrackType.TRACK_A + CS ê´€ë ¨ íƒœê·¸ ë…¸ë“œ (ìµœìš°ì„ )
        2. TrackType.TRACK_A + ë°©ì–¸ íƒœê·¸ ì—†ëŠ” ë…¸ë“œ (ë³´ì¡°)
        3. ì¤‘ë¦½ ë…¸ë“œ (TRACK_B ë° ë°©ì–¸ íƒœê·¸ ì œì™¸)
        """
        from knowledge.models import KnowledgeNode, TrackType
        import random
        
        # 1ë‹¨ê³„: TRACK_A ë…¸ë“œ ì¤‘ CS íƒœê·¸ê°€ ìˆê³  ë°©ì–¸ íƒœê·¸ê°€ ì—†ëŠ” ë…¸ë“œ (ìµœìš°ì„ )
        all_track_a = list(KnowledgeNode.objects.filter(
            track_type=TrackType.TRACK_A
        ))
        
        # Tier 1: CS íƒœê·¸ ìˆìŒ + ë°©ì–¸ íƒœê·¸ ì—†ìŒ
        tier1_candidates = [
            node for node in all_track_a 
            if self._has_cs_tags(node) and not self._has_dialect_tags(node)
        ]
        
        # Tier 2: CS íƒœê·¸ ì—†ì§€ë§Œ ë°©ì–¸ íƒœê·¸ë„ ì—†ìŒ (ì¤‘ë¦½ ë…¸ë“œ)
        tier2_candidates = [
            node for node in all_track_a 
            if not self._has_cs_tags(node) and not self._has_dialect_tags(node)
        ]
        
        # ë¡œê¹…
        logger.info(f"[NodeSelector] TRACK_A ë¶„ì„: ì´ {len(all_track_a)}ê°œ, "
                   f"CSíƒœê·¸ {len(tier1_candidates)}ê°œ, ì¤‘ë¦½ {len(tier2_candidates)}ê°œ")
        
        # Tier 1ì—ì„œ ìš°ì„  ì„ ì •
        random.shuffle(tier1_candidates)
        selected = tier1_candidates[:self.nodes_per_domain]
        
        # Tier 1ì´ ë¶€ì¡±í•˜ë©´ Tier 2ì—ì„œ ì¶”ê°€
        if len(selected) < self.nodes_per_domain:
            needed = self.nodes_per_domain - len(selected)
            random.shuffle(tier2_candidates)
            selected.extend(tier2_candidates[:needed])
        
        # 2ë‹¨ê³„: ì—¬ì „íˆ ë¶€ì¡±í•˜ë©´ ë‹¤ë¥¸ ë…¸ë“œì—ì„œ ì¶”ê°€ (TRACK_B ë° ë°©ì–¸ íƒœê·¸ ì œì™¸)
        if len(selected) < self.nodes_per_domain:
            existing_ids = {node.id for node in selected}
            
            additional_candidates = [
                node for node in KnowledgeNode.objects.exclude(
                    id__in=existing_ids
                ).exclude(
                    track_type=TrackType.TRACK_B
                )
                if not self._has_dialect_tags(node)
            ]
            
            random.shuffle(additional_candidates)
            needed = self.nodes_per_domain - len(selected)
            selected.extend(additional_candidates[:needed])
        
        logger.info(f"[NodeSelector] CS ë„ë©”ì¸ ë…¸ë“œ {len(selected)}ê°œ ì„ ì • (TRACK_A ê¸°ë°˜, ë°©ì–¸ íƒœê·¸ ì œì™¸)")
        return selected
    
    def select_dialect_nodes(self) -> List:
        """
        ê²½ìƒë„/ì „ë¼ë„ ì‚¬íˆ¬ë¦¬ ë„ë©”ì¸ ë…¸ë“œ ì„ ì •
        
        ì„ ì • ê¸°ì¤€:
        1. TrackType.TRACK_Bì¸ ë…¸ë“œ ìš°ì„ 
        2. ë¶€ì¡±í•˜ë©´ ë°©ì–¸/ì‚¬íˆ¬ë¦¬ ê´€ë ¨ íƒœê·¸ê°€ ìˆëŠ” TRACK_A ë…¸ë“œë„ í¬í•¨
        """
        from knowledge.models import KnowledgeNode, TrackType
        import random
        
        # 1ë‹¨ê³„: TRACK_B ë…¸ë“œ ì„ ì •
        track_b_nodes = list(KnowledgeNode.objects.filter(
            track_type=TrackType.TRACK_B
        ))
        random.shuffle(track_b_nodes)
        selected = track_b_nodes[:self.nodes_per_domain]
        
        # 2ë‹¨ê³„: ë¶€ì¡±í•˜ë©´ TRACK_A ì¤‘ ë°©ì–¸ íƒœê·¸ê°€ ìˆëŠ” ë…¸ë“œ ì¶”ê°€
        if len(selected) < self.nodes_per_domain:
            existing_ids = {node.id for node in selected}
            
            dialect_tagged_nodes = [
                node for node in KnowledgeNode.objects.filter(
                    track_type=TrackType.TRACK_A
                ).exclude(id__in=existing_ids)
                if self._has_dialect_tags(node)
            ]
            
            random.shuffle(dialect_tagged_nodes)
            needed = self.nodes_per_domain - len(selected)
            selected.extend(dialect_tagged_nodes[:needed])
            
            if dialect_tagged_nodes:
                logger.info(f"[NodeSelector] TRACK_Aì—ì„œ ë°©ì–¸ íƒœê·¸ ë…¸ë“œ {min(needed, len(dialect_tagged_nodes))}ê°œ ì¶”ê°€ ì„ ì •")
        
        logger.info(f"[NodeSelector] ì‚¬íˆ¬ë¦¬ ë„ë©”ì¸ ë…¸ë“œ {len(selected)}ê°œ ì„ ì • (TRACK_B + ë°©ì–¸ íƒœê·¸)")
        return selected
    
    def get_test_nodes(self) -> Dict[str, List]:
        """í…ŒìŠ¤íŠ¸ìš© ì „ì²´ ë…¸ë“œ ì„¸íŠ¸ ë°˜í™˜"""
        cs_nodes = self.select_cs_nodes()
        dialect_nodes = self.select_dialect_nodes()
        
        # ì¤‘ë³µ ê²€ì¦: ê°™ì€ ë…¸ë“œê°€ ì–‘ìª½ì— í¬í•¨ë˜ì–´ ìˆì§€ ì•Šì€ì§€ í™•ì¸
        cs_ids = {node.id for node in cs_nodes}
        dialect_ids = {node.id for node in dialect_nodes}
        overlap = cs_ids & dialect_ids
        
        if overlap:
            logger.warning(f"[NodeSelector] CS/Dialect ì¤‘ë³µ ë…¸ë“œ {len(overlap)}ê°œ ë°œê²¬, Dialectì—ì„œ ì œê±°")
            dialect_nodes = [node for node in dialect_nodes if node.id not in overlap]
        
        return {
            Domain.CS.value: cs_nodes,
            Domain.DIALECT.value: dialect_nodes
        }


# =============================================================================
# í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ìƒì„±ê¸°
# =============================================================================

class SessionScheduler:
    """
    í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ìŠ¤ì¼€ì¤„ ìƒì„±
    
    ì—ë¹™í•˜ìš°ìŠ¤ ë§ê° ê³¡ì„  ê¸°ë°˜ ì‹œê°„ ê°„ê²©:
    - T0: ì¦‰ì‹œ (ì´ˆê¸° ì¸ì§€ í…ŒìŠ¤íŠ¸)
    - T1: 10ë¶„ í›„
    - T2: 1ì‹œê°„ í›„
    - T3: 24ì‹œê°„ í›„
    """
    
    # ì‹œê°„ ê°„ê²© ì •ì˜ (ë¶„ ë‹¨ìœ„)
    TIME_INTERVALS = {
        'T0': 0,
        'T1': 10,      # 10ë¶„
        'T2': 60,      # 1ì‹œê°„
        'T3': 1440,    # 24ì‹œê°„
    }
    
    def create_sessions(self, user, start_time: datetime = None) -> Dict:
        """
        ì‚¬ìš©ìì˜ í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ìƒì„±
        
        Args:
            user: User ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
            start_time: ì‹œì‘ ì‹œê°„ (ê¸°ë³¸: í˜„ì¬)
            
        Returns:
            ì‹œì ë³„ ì„¸ì…˜ ë”•ì…”ë„ˆë¦¬
        """
        from analytics.models import TestSession, TimePoint
        
        if start_time is None:
            start_time = timezone.now()
        
        sessions = {}
        
        for time_point, minutes in self.TIME_INTERVALS.items():
            scheduled_at = start_time + timedelta(minutes=minutes)
            
            session = TestSession.objects.create(
                user=user,
                time_point=time_point,
                scheduled_at=scheduled_at
            )
            sessions[time_point] = session
            
            logger.info(f"[SessionScheduler] ì„¸ì…˜ ìƒì„±: {time_point} @ {scheduled_at}")
        
        return sessions


# =============================================================================
# í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë¡ê¸°
# =============================================================================

class ResultRecorder:
    """
    í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë¡
    
    ê° í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ì €ì¥í•˜ê³  ë°œí™” ë¶„ì„ ë©”íƒ€ë°ì´í„°ë¥¼ ê´€ë¦¬
    """
    
    def record_result(
        self,
        session,
        node,
        is_correct: bool,
        confidence_score: float,
        response_time_ms: int,
        test_type: str,
        speech_data: Optional[Dict] = None
    ):
        """
        í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë¡
        
        Args:
            session: TestSession ì¸ìŠ¤í„´ìŠ¤
            node: KnowledgeNode ì¸ìŠ¤í„´ìŠ¤
            is_correct: ì •ë‹µ ì—¬ë¶€
            confidence_score: í™•ì‹ ë„ (0~1)
            response_time_ms: ë°˜ì‘ ì‹œê°„ (ms)
            test_type: í…ŒìŠ¤íŠ¸ ìœ í˜• (A1_BOTTOM_UP, A2_TOP_DOWN, B_RECALL)
            speech_data: A2 í…ŒìŠ¤íŠ¸ ì‹œ ë°œí™” ë¶„ì„ ë°ì´í„°
        """
        from analytics.models import TestResult, SpeechAnalysis, TestType
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìƒì„± (illusion_scoreëŠ” ìë™ ê³„ì‚°ë¨)
        result = TestResult.objects.create(
            session=session,
            node=node,
            is_correct=is_correct,
            confidence_score=confidence_score,
            response_time_ms=response_time_ms,
            test_type=test_type
        )
        
        # A2 í…ŒìŠ¤íŠ¸ì˜ ê²½ìš° ë°œí™” ë¶„ì„ ì €ì¥
        if test_type == TestType.A2_TOP_DOWN and speech_data:
            SpeechAnalysis.objects.create(
                result=result,
                pause_count=speech_data.get('pause_count', 0),
                total_pause_duration=speech_data.get('total_pause_duration', 0),
                speech_segments=speech_data.get('speech_segments', 0),
                text_length=speech_data.get('text_length', 0)
            )
        
        # ë…¸ë“œì˜ ì•ˆì •ì„±/ë‚œì´ë„ ì§€ìˆ˜ ì—…ë°ì´íŠ¸
        node.update_stability_index(response_time_ms, is_correct)
        node.update_difficulty_index(response_time_ms, is_correct)
        
        logger.debug(f"[ResultRecorder] ê²°ê³¼ ê¸°ë¡: {node.title} @ {session.time_point}")
        
        return result


# =============================================================================
# ë§ê° ê³¡ì„  ë¶„ì„ê¸°
# =============================================================================

class ForgettingCurveAnalyzer:
    """
    ë§ê° ê³¡ì„  ë¶„ì„
    
    ì‹œê°„ ê²½ê³¼ì— ë”°ë¥¸ ì •ë‹µë¥  ë³€í™”ë¥¼ ë¶„ì„í•˜ì—¬ ë§ê° ê¸°ìš¸ê¸° k ì¶”ì •
    ì—ë¹™í•˜ìš°ìŠ¤ ë§ê° ê³¡ì„ : R(t) = e^(-t/S) ë˜ëŠ” R(t) = e^(-k*t)
    """
    
    # ì‹œê°„ ê°„ê²© (ë¶„ â†’ ì‹œê°„)
    TIME_POINTS_HOURS = {
        'T0': 0,
        'T1': 10/60,    # ~0.167ì‹œê°„
        'T2': 1,        # 1ì‹œê°„
        'T3': 24,       # 24ì‹œê°„
    }
    
    def calculate_forgetting_k(self, time_point_stats: List[TimePointStats]) -> float:
        """
        ë§ê° ê¸°ìš¸ê¸° k ê³„ì‚°
        
        ì„ í˜• íšŒê·€ë¥¼ ì‚¬ìš©í•˜ì—¬ log(R) = -k*t + log(R0) í˜•íƒœë¡œ ì¶”ì •
        R: ìœ ì§€ìœ¨ (ì •ë‹µë¥ )
        t: ì‹œê°„ (ì‹œê°„ ë‹¨ìœ„)
        
        Args:
            time_point_stats: ì‹œì ë³„ í†µê³„ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ë§ê° ê¸°ìš¸ê¸° k (í´ìˆ˜ë¡ ë¹ ë¥´ê²Œ ë§ê°)
        """
        if len(time_point_stats) < 2:
            return 0.5  # ê¸°ë³¸ê°’
        
        # ë°ì´í„° í¬ì¸íŠ¸ ì¶”ì¶œ
        points = []
        for stat in time_point_stats:
            t = self.TIME_POINTS_HOURS.get(stat.time_point, 0)
            r = max(stat.accuracy, 0.01)  # log(0) ë°©ì§€
            points.append((t, math.log(r)))
        
        # ì„ í˜• íšŒê·€ (ìµœì†Œì œê³±ë²•)
        n = len(points)
        sum_t = sum(p[0] for p in points)
        sum_log_r = sum(p[1] for p in points)
        sum_t2 = sum(p[0]**2 for p in points)
        sum_t_log_r = sum(p[0] * p[1] for p in points)
        
        denominator = n * sum_t2 - sum_t**2
        if abs(denominator) < 1e-10:
            return 0.5  # ê¸°ë³¸ê°’
        
        # k = -slope
        slope = (n * sum_t_log_r - sum_t * sum_log_r) / denominator
        k = -slope
        
        # ìŒìˆ˜ ë°©ì§€ ë° ë²”ìœ„ ì œí•œ (0.01 ~ 5.0)
        k = max(0.01, min(k, 5.0))
        
        logger.info(f"[ForgettingCurve] k = {k:.4f}")
        return k
    
    def calculate_encoding_strength(self, t0_stats: TimePointStats) -> float:
        """
        ì´ˆê¸° ê¸°ì–µ í˜•ì„± ê°•ë„ ê³„ì‚°
        
        T0 ì‹œì ì˜ ì •ë‹µë¥ ê³¼ RTë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¸ì½”ë”© ê°•ë„ ì¶”ì •
        
        Args:
            t0_stats: T0 ì‹œì  í†µê³„
            
        Returns:
            ì¸ì½”ë”© ê°•ë„ (0~1)
        """
        # ì •ë‹µë¥  ê¸°ì—¬
        accuracy_factor = t0_stats.accuracy
        
        # RT ê¸°ì—¬ (ë¹ ë¥¼ìˆ˜ë¡ ê°•í•¨, 5ì´ˆë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”)
        rt_factor = 1 - min(t0_stats.avg_rt_ms / 5000, 1.0)
        
        # í™•ì‹ ë„ ê¸°ì—¬
        confidence_factor = t0_stats.avg_confidence
        
        # ê°€ì¤‘ í‰ê· 
        encoding_strength = (
            0.5 * accuracy_factor + 
            0.3 * rt_factor + 
            0.2 * confidence_factor
        )
        
        return encoding_strength
    
    def analyze_domain(
        self, 
        user_id: str, 
        domain: Domain, 
        node_ids: List[str]
    ) -> DomainAnalysis:
        """
        ë„ë©”ì¸ë³„ ë¶„ì„ ìˆ˜í–‰
        
        Args:
            user_id: ì‚¬ìš©ì ID
            domain: ë„ë©”ì¸ ìœ í˜•
            node_ids: í•´ë‹¹ ë„ë©”ì¸ì˜ ë…¸ë“œ ID ë¦¬ìŠ¤íŠ¸
            
        Returns:
            DomainAnalysis ê²°ê³¼
        """
        from analytics.models import TestResult, TestSession, TimePoint
        
        time_point_stats = []
        
        for tp in ['T0', 'T1', 'T2', 'T3']:
            # í•´ë‹¹ ì‹œì ì˜ ê²°ê³¼ ì¡°íšŒ
            results = TestResult.objects.filter(
                session__user_id=user_id,
                session__time_point=tp,
                node_id__in=node_ids
            )
            
            if not results.exists():
                continue
            
            # í†µê³„ ê³„ì‚°
            stats = results.aggregate(
                accuracy=Avg('is_correct', output_field=models.FloatField()),
                avg_rt=Avg('response_time_ms'),
                avg_conf=Avg('confidence_score'),
                avg_illusion=Avg('illusion_score')
            )
            
            time_point_stats.append(TimePointStats(
                time_point=tp,
                accuracy=float(stats['accuracy'] or 0),
                avg_rt_ms=float(stats['avg_rt'] or 0),
                avg_confidence=float(stats['avg_conf'] or 0),
                avg_illusion=float(stats['avg_illusion'] or 0),
                sample_count=results.count()
            ))
        
        # ë§ê° ê¸°ìš¸ê¸° ê³„ì‚°
        forgetting_k = self.calculate_forgetting_k(time_point_stats)
        
        # ì´ˆê¸° ì¸ì½”ë”© ê°•ë„
        t0_stats = next((s for s in time_point_stats if s.time_point == 'T0'), None)
        encoding_strength = self.calculate_encoding_strength(t0_stats) if t0_stats else 0.5
        
        # T3 ìœ ì§€ìœ¨
        t3_stats = next((s for s in time_point_stats if s.time_point == 'T3'), None)
        retention_rate_t3 = t3_stats.accuracy if t3_stats else 0
        
        # í‰ê·  ì°©ê° ì§€ìˆ˜
        all_illusion = [s.avg_illusion for s in time_point_stats if s.avg_illusion is not None]
        avg_illusion = sum(all_illusion) / len(all_illusion) if all_illusion else 0
        
        # ì°©ê° ì„±í–¥ íŒì •
        if avg_illusion > 0.1:
            illusion_tendency = "ê³¼ì‹  (Overconfident)"
        elif avg_illusion < -0.1:
            illusion_tendency = "ì‹ ì¤‘ (Underconfident)"
        else:
            illusion_tendency = "ê· í˜• (Calibrated)"
        
        return DomainAnalysis(
            domain=domain.value,
            time_point_stats=time_point_stats,
            forgetting_k=forgetting_k,
            encoding_strength=encoding_strength,
            retention_rate_t3=retention_rate_t3,
            avg_illusion_score=avg_illusion,
            illusion_tendency=illusion_tendency
        )


# =============================================================================
# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ê¸°
# =============================================================================

class CognitiveBenchmark:
    """
    ì´ˆê¸° ì¸ì§€ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°
    
    ë§ê° ê³¡ì„  ì¶”ì •ì„ ìœ„í•œ ì „ì²´ í…ŒìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸:
    1. ë…¸ë“œ ì„ ì •
    2. ì„¸ì…˜ ìŠ¤ì¼€ì¤„ë§
    3. ê²°ê³¼ ê¸°ë¡
    4. ë¶„ì„ ë° ë§ê° ìƒìˆ˜ ì¶”ì •
    """
    
    def __init__(self, nodes_per_domain: int = 10):
        self.node_selector = NodeSelector(nodes_per_domain)
        self.session_scheduler = SessionScheduler()
        self.result_recorder = ResultRecorder()
        self.analyzer = ForgettingCurveAnalyzer()
    
    def initialize_benchmark(self, user) -> Dict:
        """
        ë²¤ì¹˜ë§ˆí¬ ì´ˆê¸°í™”
        
        í…ŒìŠ¤íŠ¸ ì„¸ì…˜ê³¼ ë…¸ë“œ ì„¸íŠ¸ë¥¼ ìƒì„±
        
        Args:
            user: User ì¸ìŠ¤í„´ìŠ¤
            
        Returns:
            ì´ˆê¸°í™” ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        # ë…¸ë“œ ì„ ì •
        test_nodes = self.node_selector.get_test_nodes()
        
        # ì„¸ì…˜ ìƒì„±
        sessions = self.session_scheduler.create_sessions(user)
        
        logger.info(f"[CognitiveBenchmark] ì´ˆê¸°í™” ì™„ë£Œ: {user.username}")
        
        return {
            'user_id': str(user.id),
            'sessions': sessions,
            'nodes': test_nodes,
            'cs_node_ids': [str(n.id) for n in test_nodes[Domain.CS.value]],
            'dialect_node_ids': [str(n.id) for n in test_nodes[Domain.DIALECT.value]]
        }
    
    def submit_result(
        self,
        session,
        node,
        is_correct: bool,
        confidence_score: float,
        response_time_ms: int,
        test_type: str,
        speech_data: Optional[Dict] = None
    ):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì œì¶œ"""
        return self.result_recorder.record_result(
            session=session,
            node=node,
            is_correct=is_correct,
            confidence_score=confidence_score,
            response_time_ms=response_time_ms,
            test_type=test_type,
            speech_data=speech_data
        )
    
    def analyze_results(
        self,
        user_id: str,
        cs_node_ids: List[str],
        dialect_node_ids: List[str]
    ) -> BenchmarkResult:
        """
        ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¶„ì„
        
        Args:
            user_id: ì‚¬ìš©ì ID
            cs_node_ids: CS ë„ë©”ì¸ ë…¸ë“œ ID ë¦¬ìŠ¤íŠ¸
            dialect_node_ids: ì‚¬íˆ¬ë¦¬ ë„ë©”ì¸ ë…¸ë“œ ID ë¦¬ìŠ¤íŠ¸
            
        Returns:
            BenchmarkResult
        """
        from analytics.models import User
        
        # ë„ë©”ì¸ë³„ ë¶„ì„
        cs_analysis = self.analyzer.analyze_domain(
            user_id, Domain.CS, cs_node_ids
        )
        dialect_analysis = self.analyzer.analyze_domain(
            user_id, Domain.DIALECT, dialect_node_ids
        )
        
        # ê¸°ì´ˆ ë§ê° ìƒìˆ˜ ì¶”ì • (ë‘ ë„ë©”ì¸ì˜ ê°€ì¤‘ í‰ê· )
        # ìµìˆ™í•œ ë„ë©”ì¸ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
        base_k = (0.6 * cs_analysis.forgetting_k + 
                  0.4 * dialect_analysis.forgetting_k)
        
        # ë„ë©”ì¸ ê°„ ë§ê° ë¹„ìœ¨
        domain_ratio = (dialect_analysis.forgetting_k / cs_analysis.forgetting_k 
                        if cs_analysis.forgetting_k > 0 else 1.0)
        
        # ì „ì²´ í‰ê·  ì°©ê° ì§€ìˆ˜
        overall_illusion = (cs_analysis.avg_illusion_score + 
                           dialect_analysis.avg_illusion_score) / 2
        
        # ë³µìŠµ ìŠ¤ì¼€ì¤„ ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendation = self._generate_recommendation(
            base_k, domain_ratio, overall_illusion
        )
        
        # ì‚¬ìš©ì ëª¨ë¸ ì—…ë°ì´íŠ¸
        user = User.objects.get(id=user_id)
        user.base_forgetting_k = base_k
        user.update_illusion_avg()
        user.save(update_fields=['base_forgetting_k'])
        
        return BenchmarkResult(
            user_id=user_id,
            cs_analysis=cs_analysis,
            dialect_analysis=dialect_analysis,
            base_forgetting_k=base_k,
            domain_forgetting_ratio=domain_ratio,
            overall_illusion_avg=overall_illusion,
            recommendation=recommendation
        )
    
    def _generate_recommendation(
        self,
        base_k: float,
        domain_ratio: float,
        illusion_avg: float
    ) -> str:
        """ë³µìŠµ ìŠ¤ì¼€ì¤„ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ë§ê° ì†ë„ ê¸°ë°˜ ê¶Œì¥
        if base_k > 1.0:
            recommendations.append("âš¡ ë¹ ë¥¸ ë§ê° íŒ¨í„´: ì§§ì€ ê°„ê²©ì˜ ë°˜ë³µ ë³µìŠµ ê¶Œì¥ (10ë¶„, 1ì‹œê°„, 1ì¼)")
        elif base_k > 0.3:
            recommendations.append("ğŸ“Š ë³´í†µ ë§ê° íŒ¨í„´: í‘œì¤€ ê°„ê²© ë³µìŠµ ê¶Œì¥ (1ì‹œê°„, 1ì¼, 1ì£¼)")
        else:
            recommendations.append("ğŸ§  ëŠë¦° ë§ê° íŒ¨í„´: ë„‰ë„‰í•œ ê°„ê²© ë³µìŠµ ê°€ëŠ¥ (1ì¼, 1ì£¼, 1ê°œì›”)")
        
        # ë„ë©”ì¸ ì°¨ì´ ê¸°ë°˜ ê¶Œì¥
        if domain_ratio > 1.5:
            recommendations.append("ğŸ“š ì‹ ê·œ ë„ë©”ì¸ í•™ìŠµ ì‹œ ë” ì§‘ì¤‘ì ì¸ ë³µìŠµ í•„ìš”")
        elif domain_ratio < 0.8:
            recommendations.append("âœ¨ ì‹ ê·œ ë„ë©”ì¸ë„ íš¨ê³¼ì ìœ¼ë¡œ ê¸°ì–µ ìœ ì§€ ì¤‘")
        
        # ë©”íƒ€ì¸ì§€ ê¸°ë°˜ ê¶Œì¥
        if illusion_avg > 0.15:
            recommendations.append("âš ï¸ ê³¼ì‹  ê²½í–¥: ìê°€ í…ŒìŠ¤íŠ¸ ê°•í™” ë° í™•ì‹ ë„ ì¬ì¡°ì • í•„ìš”")
        elif illusion_avg < -0.15:
            recommendations.append("ğŸ’ª ì‹ ì¤‘í•œ ê²½í–¥: ìì‹ ê° ìˆê²Œ ë³µìŠµ ê°„ê²© í™•ì¥ ê°€ëŠ¥")
        
        return " | ".join(recommendations)


# =============================================================================
# ê²°ê³¼ ë¦¬í¬í„°
# =============================================================================

class BenchmarkReporter:
    """
    ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±
    
    ì¸ì§€ íŠ¹ì„± ë¶„ì„ ê´€ì ì˜ ì„œìˆ ì  ë¦¬í¬íŠ¸
    """
    
    def generate_report(
        self, 
        result: BenchmarkResult, 
        target_retention: float = 0.8
    ) -> Dict:
        """
        ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
        
        í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì§ì ‘ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì™„ì „í•œ ë°ì´í„° ì œê³µ:
        - forgetting_curve: ë§ê°ê³¡ì„  ê·¸ë˜í”„ìš© ì¢Œí‘œ ë°ì´í„°
        - recommended_review: ë³µìŠµ íƒ€ì´ë° ì‹œê°í™” ì •ë³´
        
        Args:
            result: BenchmarkResult
            target_retention: ëª©í‘œ ì•”ê¸°ìœ¨ (ê¸°ë³¸: 0.8)
            
        Returns:
            ë¦¬í¬íŠ¸ ë”•ì…”ë„ˆë¦¬
        """
        k_cs = result.cs_analysis.forgetting_k
        k_dialect = result.dialect_analysis.forgetting_k
        
        # ë³µìŠµ ìŠ¤ì¼€ì¤„ ê³„ì‚°
        schedule_calculator = ReviewScheduleCalculator()
        review_schedule = schedule_calculator.calculate_review_schedule(
            k_cs=k_cs,
            k_dialect=k_dialect,
            target_retention=target_retention
        )
        
        # ë§ê°ê³¡ì„  ë°ì´í„° ìƒì„±
        curve_generator = ForgettingCurveGenerator()
        forgetting_curve = curve_generator.to_dict(k_cs, k_dialect)
        
        return {
            'summary': {
                'user_id': result.user_id,
                'base_forgetting_k': round(result.base_forgetting_k, 4),
                'k_cs': round(k_cs, 4),
                'k_dialect': round(k_dialect, 4),
                'domain_ratio': round(result.domain_forgetting_ratio, 2),
                'overall_illusion': round(result.overall_illusion_avg, 3),
                'recommendation': result.recommendation
            },
            
            # =========================================================
            # ë§ê°ê³¡ì„  ì‹œê°í™” ë°ì´í„° (í”„ë¡ íŠ¸ì—”ë“œ ê·¸ë˜í”„ìš©)
            # =========================================================
            'forgetting_curve': forgetting_curve,
            
            # =========================================================
            # ë³µìŠµ íƒ€ì´ë° ì‹œê°í™” ì •ë³´
            # =========================================================
            'recommended_review': {
                'target_retention': target_retention,
                'cs': {
                    'hours': review_schedule.cs_review_hours,
                    'label': schedule_calculator.format_hours_to_human_readable(
                        review_schedule.cs_review_hours
                    ),
                    'curve_x': review_schedule.cs_review_hours  # ê·¸ë˜í”„ xì¢Œí‘œ
                },
                'dialect': {
                    'hours': review_schedule.dialect_review_hours,
                    'label': schedule_calculator.format_hours_to_human_readable(
                        review_schedule.dialect_review_hours
                    ),
                    'curve_x': review_schedule.dialect_review_hours  # ê·¸ë˜í”„ xì¢Œí‘œ
                }
            },
            
            # =========================================================
            # ë„ë©”ì¸ë³„ ìƒì„¸ ë¶„ì„
            # =========================================================
            'cs_domain': self._format_domain_report(result.cs_analysis, "Computer Science"),
            'dialect_domain': self._format_domain_report(result.dialect_analysis, "ê²½ìƒë„ ì‚¬íˆ¬ë¦¬"),
            'temporal_comparison': self._format_temporal_comparison(
                result.cs_analysis, result.dialect_analysis
            ),
            'cognitive_interpretation': self._generate_interpretation(result)
        }
    
    def _format_domain_report(self, analysis: DomainAnalysis, domain_name: str) -> Dict:
        """ë„ë©”ì¸ë³„ ë¦¬í¬íŠ¸ í¬ë§·"""
        return {
            'domain': domain_name,
            'forgetting_k': round(analysis.forgetting_k, 4),
            'encoding_strength': round(analysis.encoding_strength, 3),
            'retention_rate_24h': round(analysis.retention_rate_t3, 3),
            'illusion_tendency': analysis.illusion_tendency,
            'time_series': [
                {
                    'time_point': s.time_point,
                    'accuracy': round(s.accuracy, 3),
                    'avg_rt_ms': round(s.avg_rt_ms, 1),
                    'avg_confidence': round(s.avg_confidence, 3),
                    'illusion_score': round(s.avg_illusion, 3),
                    'sample_count': s.sample_count
                }
                for s in analysis.time_point_stats
            ]
        }
    
    def _format_temporal_comparison(
        self, 
        cs: DomainAnalysis, 
        dialect: DomainAnalysis
    ) -> Dict:
        """ì‹œê°„ì¶• ë¹„êµ ë°ì´í„°"""
        comparison = {}
        
        for tp in ['T0', 'T1', 'T2', 'T3']:
            cs_stat = next((s for s in cs.time_point_stats if s.time_point == tp), None)
            dialect_stat = next((s for s in dialect.time_point_stats if s.time_point == tp), None)
            
            comparison[tp] = {
                'cs_accuracy': round(cs_stat.accuracy, 3) if cs_stat else None,
                'dialect_accuracy': round(dialect_stat.accuracy, 3) if dialect_stat else None,
                'accuracy_difference': (
                    round(cs_stat.accuracy - dialect_stat.accuracy, 3)
                    if cs_stat and dialect_stat else None
                ),
                'cs_rt_ms': round(cs_stat.avg_rt_ms, 1) if cs_stat else None,
                'dialect_rt_ms': round(dialect_stat.avg_rt_ms, 1) if dialect_stat else None,
            }
        
        return comparison
    
    def _generate_interpretation(self, result: BenchmarkResult) -> Dict:
        """ì¸ì§€ íŠ¹ì„± í•´ì„"""
        cs = result.cs_analysis
        dialect = result.dialect_analysis
        
        return {
            'forgetting_pattern': self._interpret_forgetting(cs, dialect),
            'encoding_pattern': self._interpret_encoding(cs, dialect),
            'metacognition_pattern': self._interpret_metacognition(cs, dialect),
            'domain_transfer': self._interpret_domain_transfer(result.domain_forgetting_ratio)
        }
    
    def _interpret_forgetting(self, cs: DomainAnalysis, dialect: DomainAnalysis) -> str:
        """ë§ê° íŒ¨í„´ í•´ì„"""
        k_diff = dialect.forgetting_k - cs.forgetting_k
        
        if k_diff > 0.3:
            return (
                f"CS ë„ë©”ì¸(k={cs.forgetting_k:.3f})ì— ë¹„í•´ "
                f"ì‚¬íˆ¬ë¦¬ ë„ë©”ì¸(k={dialect.forgetting_k:.3f})ì˜ ë§ê°ì´ "
                f"{k_diff:.3f}ë§Œí¼ ë¹ ë¦„. ì´ëŠ” ì‚¬ì „ ì§€ì‹ì´ ì—†ëŠ” ìƒˆë¡œìš´ ì •ë³´ê°€ "
                f"ë” ë¹ ë¥´ê²Œ ë¶•ê´´ë˜ëŠ” ì¸ì§€ì  íŠ¹ì„±ì„ ë°˜ì˜í•¨."
            )
        elif k_diff < -0.1:
            return (
                f"í¥ë¯¸ë¡­ê²Œë„ ì‚¬íˆ¬ë¦¬ ë„ë©”ì¸(k={dialect.forgetting_k:.3f})ì´ "
                f"CS ë„ë©”ì¸(k={cs.forgetting_k:.3f})ë³´ë‹¤ ê¸°ì–µ ìœ ì§€ìœ¨ì´ ë†’ìŒ. "
                f"ì´ëŠ” ì •ì„œì  ì—°ê²° ë˜ëŠ” ì¼í™”ì  ê¸°ì–µ íš¨ê³¼ì¼ ìˆ˜ ìˆìŒ."
            )
        else:
            return (
                f"ë‘ ë„ë©”ì¸ì˜ ë§ê° ê¸°ìš¸ê¸°ê°€ ìœ ì‚¬í•¨ "
                f"(CS: {cs.forgetting_k:.3f}, ì‚¬íˆ¬ë¦¬: {dialect.forgetting_k:.3f}). "
                f"ë„ë©”ì¸ ì¹œìˆ™ë„ì™€ ë¬´ê´€í•˜ê²Œ ì¼ì •í•œ ê¸°ì–µ íŒ¨í„´ì„ ë³´ì„."
            )
    
    def _interpret_encoding(self, cs: DomainAnalysis, dialect: DomainAnalysis) -> str:
        """ì¸ì½”ë”© íŒ¨í„´ í•´ì„"""
        enc_diff = cs.encoding_strength - dialect.encoding_strength
        
        if enc_diff > 0.15:
            return (
                f"CS ë„ë©”ì¸ì˜ ì´ˆê¸° ì¸ì½”ë”© ê°•ë„({cs.encoding_strength:.3f})ê°€ "
                f"ì‚¬íˆ¬ë¦¬ ë„ë©”ì¸({dialect.encoding_strength:.3f})ë³´ë‹¤ ë†’ìŒ. "
                f"ê¸°ì¡´ ìŠ¤í‚¤ë§ˆì™€ì˜ ì—°ê²°ì´ ì´ˆê¸° ê¸°ì–µ í˜•ì„±ì„ ê°•í™”í•¨."
            )
        else:
            return (
                f"ë‘ ë„ë©”ì¸ì˜ ì´ˆê¸° ì¸ì½”ë”© ê°•ë„ê°€ ìœ ì‚¬í•¨ "
                f"(CS: {cs.encoding_strength:.3f}, ì‚¬íˆ¬ë¦¬: {dialect.encoding_strength:.3f}). "
                f"í•™ìŠµ ì‹œì ì˜ ì£¼ì˜ ì§‘ì¤‘ë„ê°€ ì¼ì •í•¨ì„ ì‹œì‚¬í•¨."
            )
    
    def _interpret_metacognition(self, cs: DomainAnalysis, dialect: DomainAnalysis) -> str:
        """ë©”íƒ€ì¸ì§€ íŒ¨í„´ í•´ì„"""
        cs_ill = cs.avg_illusion_score
        dialect_ill = dialect.avg_illusion_score
        
        interpretations = []
        
        if cs_ill > 0.1:
            interpretations.append(
                f"CS ë„ë©”ì¸ì—ì„œ ê³¼ì‹  ê²½í–¥(illusion={cs_ill:.2f}): "
                f"ìµìˆ™í•œ ë„ë©”ì¸ì—ì„œ ìì‹ ì˜ ê¸°ì–µì„ ê³¼ëŒ€í‰ê°€í•˜ëŠ” ê²½í–¥ì´ ìˆìŒ."
            )
        elif cs_ill < -0.1:
            interpretations.append(
                f"CS ë„ë©”ì¸ì—ì„œ ì‹ ì¤‘í•œ ê²½í–¥(illusion={cs_ill:.2f}): "
                f"ì‹¤ì œ ì •ë‹µë¥ ë³´ë‹¤ í™•ì‹ ë„ê°€ ë‚®ì•„ ë³´ìˆ˜ì  íŒë‹¨ì„ í•¨."
            )
        
        if dialect_ill > 0.1:
            interpretations.append(
                f"ì‚¬íˆ¬ë¦¬ ë„ë©”ì¸ì—ì„œ ê³¼ì‹  ê²½í–¥(illusion={dialect_ill:.2f}): "
                f"ìƒˆë¡œìš´ ì •ë³´ì— ëŒ€í•´ì„œë„ ìì‹ ê°ì´ ë†’ìŒ."
            )
        elif dialect_ill < -0.1:
            interpretations.append(
                f"ì‚¬íˆ¬ë¦¬ ë„ë©”ì¸ì—ì„œ ì‹ ì¤‘í•œ ê²½í–¥(illusion={dialect_ill:.2f}): "
                f"ë¶ˆí™•ì‹¤í•œ ì •ë³´ì— ëŒ€í•´ ì ì ˆíˆ ë‚®ì€ í™•ì‹ ë„ë¥¼ ë³´ì„."
            )
        
        if not interpretations:
            interpretations.append(
                "ë‘ ë„ë©”ì¸ ëª¨ë‘ì—ì„œ ê· í˜• ì¡íŒ ë©”íƒ€ì¸ì§€ë¥¼ ë³´ì„. "
                "í™•ì‹ ë„ì™€ ì‹¤ì œ ì •ë‹µë¥ ì´ ì˜ ì¼ì¹˜í•¨."
            )
        
        return " ".join(interpretations)
    
    def _interpret_domain_transfer(self, ratio: float) -> str:
        """ë„ë©”ì¸ ì „ì´ í•´ì„"""
        if ratio > 1.5:
            return (
                f"ë„ë©”ì¸ ë§ê° ë¹„ìœ¨ì´ {ratio:.2f}ë¡œ, ì‹ ê·œ ë„ë©”ì¸ í•™ìŠµ ì‹œ "
                f"ê¸°ì¡´ ë„ë©”ì¸ë³´ë‹¤ ì•½ {ratio:.1f}ë°° ë¹ ë¥´ê²Œ ë§ê°ë¨. "
                f"ìƒˆë¡œìš´ ë„ë©”ì¸ í•™ìŠµ ì‹œ ê°•í™”ëœ ë³µìŠµ ìŠ¤ì¼€ì¤„ ì ìš© ê¶Œì¥."
            )
        elif ratio < 0.8:
            return (
                f"ë„ë©”ì¸ ë§ê° ë¹„ìœ¨ì´ {ratio:.2f}ë¡œ, ì‹ ê·œ ë„ë©”ì¸ì—ì„œë„ "
                f"íš¨ê³¼ì ì¸ ê¸°ì–µ ìœ ì§€ë¥¼ ë³´ì„. ë‹¤ì–‘í•œ ë„ë©”ì¸ í•™ìŠµì— ì í•©í•œ ì¸ì§€ íŠ¹ì„±."
            )
        else:
            return (
                f"ë„ë©”ì¸ ë§ê° ë¹„ìœ¨ì´ {ratio:.2f}ë¡œ ê· í˜•ì . "
                f"ë„ë©”ì¸ ì¹œìˆ™ë„ì— ë”°ë¥¸ ë§ê° ì°¨ì´ê°€ í¬ì§€ ì•ŠìŒ."
            )
