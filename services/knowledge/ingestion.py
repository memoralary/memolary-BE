"""
Ingestion Service - ë©€í‹°ëª¨ë‹¬ ì…ë ¥ ì²˜ë¦¬

í…ìŠ¤íŠ¸ ë˜ëŠ” PDF íŒŒì¼ì„ í†µí•©ëœ ë¬¸ìì—´/ì²­í¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    from services.knowledge.ingestion import IngestionService
    
    service = IngestionService()
    
    # í…ìŠ¤íŠ¸ ì…ë ¥
    chunks = service.process("ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¡œë¶€í„°...")
    
    # PDF íŒŒì¼ ì…ë ¥
    chunks = service.process("/path/to/document.pdf")
    
    # extract_nodesì™€ ì—°ê³„
    for chunk in chunks:
        result = extract_nodes(chunk)
"""

import os
import re
import logging
from pathlib import Path
from typing import List, Union, Optional, BinaryIO
from dataclasses import dataclass, field

logger = logging.getLogger(__name__)


# =============================================================================
# Exceptions
# =============================================================================

class IngestionError(Exception):
    """Ingestion ê´€ë ¨ ê¸°ë³¸ ì˜ˆì™¸"""
    pass


class PDFExtractionError(IngestionError):
    """PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨"""
    pass


class OCRRequiredError(IngestionError):
    """OCRì´ í•„ìš”í•œ ì´ë¯¸ì§€ ê¸°ë°˜ PDF"""
    pass


class UnsupportedFormatError(IngestionError):
    """ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹"""
    pass


# =============================================================================
# Data Classes
# =============================================================================

@dataclass
class IngestionResult:
    """Ingestion ê²°ê³¼"""
    chunks: List[str]               # ì²­í¬ ë¦¬ìŠ¤íŠ¸
    source_type: str                # "text", "pdf", "file"
    source_name: str                # ì†ŒìŠ¤ ì´ë¦„ (íŒŒì¼ëª… ë˜ëŠ” "direct_input")
    total_chars: int                # ì´ ë¬¸ì ìˆ˜
    page_count: int = 0             # PDF í˜ì´ì§€ ìˆ˜
    warnings: List[str] = field(default_factory=list)

    @property
    def text(self) -> str:
        """ì „ì²´ í…ìŠ¤íŠ¸ ë°˜í™˜ (ì²­í¬ ê²°í•©)"""
        return "\n\n".join(self.chunks)
    
    @property
    def chunk_count(self) -> int:
        return len(self.chunks)


# =============================================================================
# Text Cleaner
# =============================================================================

class TextCleaner:
    """í…ìŠ¤íŠ¸ ì •ì œ ìœ í‹¸ë¦¬í‹°"""
    
    @staticmethod
    def clean(text: str) -> str:
        """ê¸°ë³¸ í…ìŠ¤íŠ¸ ì •ì œ"""
        if not text:
            return ""
        
        # ì—°ì† ê³µë°± ì œê±°
        text = re.sub(r'[ \t]+', ' ', text)
        
        # 3ê°œ ì´ìƒ ì—°ì† ì¤„ë°”ê¿ˆ â†’ 2ê°œë¡œ
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # ì¤„ ì•ë’¤ ê³µë°± ì œê±°
        lines = [line.strip() for line in text.split('\n')]
        text = '\n'.join(lines)
        
        return text.strip()
    
    @staticmethod
    def remove_headers_footers(text: str) -> str:
        """í˜ì´ì§€ ë²ˆí˜¸, í—¤ë”/í‘¸í„° ì œê±°"""
        # ë…ë¦½ëœ ìˆ«ìë§Œ ìˆëŠ” ì¤„ ì œê±° (í˜ì´ì§€ ë²ˆí˜¸)
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            stripped = line.strip()
            # ìˆ«ìë§Œ ìˆëŠ” ì¤„ ì œê±°
            if stripped and stripped.isdigit():
                continue
            # "Page X", "- X -" íŒ¨í„´ ì œê±°
            if re.match(r'^(Page|í˜ì´ì§€)?\s*\d+\s*$', stripped, re.IGNORECASE):
                continue
            if re.match(r'^[-â€“â€”]\s*\d+\s*[-â€“â€”]$', stripped):
                continue
            cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines)


# =============================================================================
# PDF Extractor
# =============================================================================

class PDFExtractor:
    """PyMuPDF ê¸°ë°˜ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    
    # í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•  ë•Œ OCRì´ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨í•˜ëŠ” ì„ê³„ê°’
    MIN_CHARS_PER_PAGE = 50
    
    def __init__(self):
        self._fitz = None
    
    @property
    def fitz(self):
        """Lazy import of PyMuPDF"""
        if self._fitz is None:
            try:
                import fitz
                self._fitz = fitz
            except ImportError:
                raise ImportError(
                    "PyMuPDF íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. "
                    "'pip install pymupdf'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
                )
        return self._fitz
    
    def extract(
        self,
        source: Union[str, Path, BinaryIO],
        clean_text: bool = True
    ) -> IngestionResult:
        """
        PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        
        Args:
            source: íŒŒì¼ ê²½ë¡œ ë˜ëŠ” íŒŒì¼ ê°ì²´
            clean_text: í…ìŠ¤íŠ¸ ì •ì œ ì—¬ë¶€
            
        Returns:
            IngestionResult
        """
        warnings = []
        
        # íŒŒì¼ ì—´ê¸°
        if isinstance(source, (str, Path)):
            source_name = Path(source).name
            doc = self.fitz.open(str(source))
        else:
            # íŒŒì¼ ê°ì²´
            source_name = getattr(source, 'name', 'uploaded_file.pdf')
            file_bytes = source.read()
            doc = self.fitz.open(stream=file_bytes, filetype="pdf")
        
        try:
            page_texts = []
            low_text_pages = []
            
            for page_num, page in enumerate(doc):
                text = page.get_text()
                
                # OCR í•„ìš” ì—¬ë¶€ í™•ì¸
                if len(text.strip()) < self.MIN_CHARS_PER_PAGE:
                    low_text_pages.append(page_num + 1)
                
                if clean_text:
                    text = TextCleaner.clean(text)
                    text = TextCleaner.remove_headers_footers(text)
                
                if text.strip():
                    page_texts.append(text)
            
            # OCR ê²½ê³ 
            if low_text_pages:
                if len(low_text_pages) == len(doc):
                    # ëª¨ë“  í˜ì´ì§€ê°€ ì´ë¯¸ì§€ ê¸°ë°˜
                    warnings.append(
                        f"âš ï¸ ì´ë¯¸ì§€ ê¸°ë°˜ PDFë¡œ ë³´ì…ë‹ˆë‹¤. OCRì„ ì‚¬ìš©í•˜ì„¸ìš”. "
                        f"(ëª¨ë“  {len(doc)}í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ê°€ ê±°ì˜ ì—†ìŒ)"
                    )
                else:
                    warnings.append(
                        f"âš ï¸ ì¼ë¶€ í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ê°€ ì ìŠµë‹ˆë‹¤: {low_text_pages}. "
                        f"í•´ë‹¹ í˜ì´ì§€ëŠ” ì´ë¯¸ì§€ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                    )
            
            # í…ìŠ¤íŠ¸ê°€ ì „í˜€ ì—†ëŠ” ê²½ìš°
            if not page_texts:
                raise OCRRequiredError(
                    "PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                    "ì´ë¯¸ì§€ ê¸°ë°˜ PDFì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. OCRì´ í•„ìš”í•©ë‹ˆë‹¤."
                )
            
            return IngestionResult(
                chunks=page_texts,
                source_type="pdf",
                source_name=source_name,
                total_chars=sum(len(t) for t in page_texts),
                page_count=len(doc),
                warnings=warnings
            )
        
        finally:
            doc.close()


# =============================================================================
# Ingestion Service
# =============================================================================

class IngestionService:
    """
    ë©€í‹°ëª¨ë‹¬ ì…ë ¥ ì²˜ë¦¬ ì„œë¹„ìŠ¤
    
    í…ìŠ¤íŠ¸ ë˜ëŠ” PDF íŒŒì¼ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Example:
        service = IngestionService(chunk_size=2000)
        
        # í…ìŠ¤íŠ¸
        result = service.process("ë¨¸ì‹ ëŸ¬ë‹ì€...")
        
        # PDF
        result = service.process("/path/to/document.pdf")
        
        # extract_nodesì™€ ì—°ê³„
        for chunk in result.chunks:
            nodes = extract_nodes(chunk)
    """
    
    SUPPORTED_EXTENSIONS = {'.pdf', '.txt', '.md'}
    
    def __init__(
        self,
        chunk_size: int = 4000,  # ì†ë„ ìµœì í™”: 2000 â†’ 4000 (API í˜¸ì¶œ ì ˆë°˜)
        chunk_overlap: int = 200,
        min_chunk_size: int = 100
    ):
        """
        Args:
            chunk_size: ì²­í¬ ìµœëŒ€ ë¬¸ì ìˆ˜
            chunk_overlap: ì²­í¬ ê°„ ê²¹ì¹¨ ë¬¸ì ìˆ˜
            min_chunk_size: ìµœì†Œ ì²­í¬ í¬ê¸° (ì´ë³´ë‹¤ ì‘ìœ¼ë©´ ì´ì „ ì²­í¬ì— ë³‘í•©)
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.min_chunk_size = min_chunk_size
        self.pdf_extractor = PDFExtractor()
        self.text_cleaner = TextCleaner()
    
    def process(
        self,
        source: Union[str, Path, BinaryIO],
        *,
        chunk: bool = True,
        clean: bool = True
    ) -> IngestionResult:
        """
        ì…ë ¥ì„ ì²˜ë¦¬í•˜ì—¬ ì²­í¬ë¡œ ë°˜í™˜
        
        Args:
            source: í…ìŠ¤íŠ¸ ë¬¸ìì—´, íŒŒì¼ ê²½ë¡œ, ë˜ëŠ” íŒŒì¼ ê°ì²´
            chunk: ì²­í¬ë¡œ ë¶„í• í• ì§€ ì—¬ë¶€
            clean: í…ìŠ¤íŠ¸ ì •ì œ ì—¬ë¶€
            
        Returns:
            IngestionResult
        """
        # ì…ë ¥ íƒ€ì… íŒë³„
        if isinstance(source, (Path, BinaryIO)):
            return self._process_file(source, chunk=chunk, clean=clean)
        
        if isinstance(source, str):
            # íŒŒì¼ ê²½ë¡œì¸ì§€ í™•ì¸
            if os.path.isfile(source):
                return self._process_file(source, chunk=chunk, clean=clean)
            else:
                # ì§ì ‘ ì…ë ¥ í…ìŠ¤íŠ¸
                return self._process_text(source, chunk=chunk, clean=clean)
        
        raise UnsupportedFormatError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì…ë ¥ íƒ€ì…: {type(source)}")
    
    def _process_text(
        self,
        text: str,
        *,
        chunk: bool = True,
        clean: bool = True
    ) -> IngestionResult:
        """í…ìŠ¤íŠ¸ ì²˜ë¦¬"""
        if clean:
            text = self.text_cleaner.clean(text)
        
        if chunk:
            chunks = self._split_into_chunks(text)
        else:
            chunks = [text] if text.strip() else []
        
        return IngestionResult(
            chunks=chunks,
            source_type="text",
            source_name="direct_input",
            total_chars=len(text),
            page_count=0
        )
    
    def _process_file(
        self,
        source: Union[str, Path, BinaryIO],
        *,
        chunk: bool = True,
        clean: bool = True
    ) -> IngestionResult:
        """íŒŒì¼ ì²˜ë¦¬"""
        # í™•ì¥ì í™•ì¸
        if isinstance(source, (str, Path)):
            ext = Path(source).suffix.lower()
            source_name = Path(source).name
        else:
            # íŒŒì¼ ê°ì²´
            source_name = getattr(source, 'name', 'uploaded_file')
            ext = Path(source_name).suffix.lower()
        
        if ext == '.pdf':
            result = self.pdf_extractor.extract(source, clean_text=clean)
            
            if chunk:
                # í˜ì´ì§€ë³„ ì²­í¬ë¥¼ ì¶”ê°€ë¡œ ë¶„í• 
                all_chunks = []
                for page_text in result.chunks:
                    all_chunks.extend(self._split_into_chunks(page_text))
                result.chunks = all_chunks
            
            return result
        
        elif ext in {'.txt', '.md'}:
            # í…ìŠ¤íŠ¸ íŒŒì¼
            if isinstance(source, (str, Path)):
                with open(source, 'r', encoding='utf-8') as f:
                    text = f.read()
            else:
                text = source.read()
                if isinstance(text, bytes):
                    text = text.decode('utf-8')
            
            if clean:
                text = self.text_cleaner.clean(text)
            
            if chunk:
                chunks = self._split_into_chunks(text)
            else:
                chunks = [text] if text.strip() else []
            
            return IngestionResult(
                chunks=chunks,
                source_type="file",
                source_name=source_name,
                total_chars=len(text),
                page_count=0
            )
        
        else:
            raise UnsupportedFormatError(
                f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {ext}. "
                f"ì§€ì› í˜•ì‹: {self.SUPPORTED_EXTENSIONS}"
            )
    
    def _split_into_chunks(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        
        ë‹¨ë½(ë¹ˆ ì¤„) ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•˜ê³ , ìµœëŒ€ í¬ê¸°ë¥¼ ì´ˆê³¼í•˜ë©´ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        """
        if not text or not text.strip():
            return []
        
        # ë‹¨ë½ ê¸°ì¤€ ë¶„í• 
        paragraphs = re.split(r'\n\n+', text)
        
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
            
            # í˜„ì¬ ì²­í¬ + ìƒˆ ë‹¨ë½ì´ ìµœëŒ€ í¬ê¸° ì´ë‚´
            if len(current_chunk) + len(para) + 2 <= self.chunk_size:
                if current_chunk:
                    current_chunk += "\n\n" + para
                else:
                    current_chunk = para
            else:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                if current_chunk:
                    chunks.append(current_chunk)
                
                # ë‹¨ë½ ìì²´ê°€ ìµœëŒ€ í¬ê¸° ì´ˆê³¼ì‹œ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
                if len(para) > self.chunk_size:
                    sentences = self._split_by_sentence(para)
                    for sent in sentences:
                        if len(current_chunk) + len(sent) + 1 <= self.chunk_size:
                            if current_chunk:
                                current_chunk += " " + sent
                            else:
                                current_chunk = sent
                        else:
                            if current_chunk:
                                chunks.append(current_chunk)
                            current_chunk = sent
                else:
                    current_chunk = para
        
        # ë§ˆì§€ë§‰ ì²­í¬
        if current_chunk:
            # ìµœì†Œ í¬ê¸° ë¯¸ë‹¬ì´ë©´ ì´ì „ ì²­í¬ì— ë³‘í•©
            if len(current_chunk) < self.min_chunk_size and chunks:
                chunks[-1] += "\n\n" + current_chunk
            else:
                chunks.append(current_chunk)
        
        return chunks
    
    def _split_by_sentence(self, text: str) -> List[str]:
        """ë¬¸ì¥ ë‹¨ìœ„ ë¶„í• """
        # í•œêµ­ì–´/ì˜ì–´ ë¬¸ì¥ êµ¬ë¶„ì
        sentences = re.split(r'(?<=[.!?ã€‚])\s+', text)
        return [s.strip() for s in sentences if s.strip()]


# =============================================================================
# Convenience Functions
# =============================================================================

def ingest(
    source: Union[str, Path, BinaryIO],
    chunk_size: int = 2000
) -> List[str]:
    """
    ê°„í¸í•œ ingestion í•¨ìˆ˜
    
    Args:
        source: í…ìŠ¤íŠ¸, íŒŒì¼ ê²½ë¡œ, ë˜ëŠ” íŒŒì¼ ê°ì²´
        chunk_size: ì²­í¬ ìµœëŒ€ í¬ê¸°
        
    Returns:
        ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    service = IngestionService(chunk_size=chunk_size)
    result = service.process(source)
    
    # ê²½ê³  ì¶œë ¥
    for warning in result.warnings:
        logger.warning(warning)
    
    return result.chunks


def ingest_text(text: str, chunk_size: int = 2000) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
    return ingest(text, chunk_size=chunk_size)


def ingest_pdf(pdf_path: str, chunk_size: int = 2000) -> List[str]:
    """PDFë¥¼ ì²­í¬ë¡œ ë¶„í• """
    if not os.path.isfile(pdf_path):
        raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
    return ingest(pdf_path, chunk_size=chunk_size)


# =============================================================================
# Test
# =============================================================================

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ”§ IngestionService í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    service = IngestionService(chunk_size=500)
    
    # í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸
    sample_text = """
    ë¨¸ì‹ ëŸ¬ë‹(Machine Learning)ì€ ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ë¡œ,
    ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ì˜ˆì¸¡í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë‹¤.
    
    ì§€ë„ í•™ìŠµì€ ì •ë‹µì´ ìˆëŠ” ë°ì´í„°ë¡œ í•™ìŠµí•œë‹¤.
    ë¹„ì§€ë„ í•™ìŠµì€ ì •ë‹µ ì—†ì´ íŒ¨í„´ì„ ë°œê²¬í•œë‹¤.
    
    ë”¥ëŸ¬ë‹ì€ ì‹¬ì¸µ ì‹ ê²½ë§ì„ ì‚¬ìš©í•œë‹¤.
    """
    
    result = service.process(sample_text)
    
    print(f"\nğŸ“Š ê²°ê³¼:")
    print(f"   ì†ŒìŠ¤: {result.source_type}")
    print(f"   ì²­í¬ ìˆ˜: {result.chunk_count}")
    print(f"   ì´ ë¬¸ì: {result.total_chars}")
    
    print(f"\nğŸ“¦ ì²­í¬:")
    for i, chunk in enumerate(result.chunks, 1):
        print(f"   [{i}] {chunk[:50]}...")
    
    print("\nğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
